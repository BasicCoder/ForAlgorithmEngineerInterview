# 凸函数
## 定义
## 判定方法
# 凸优化（重点）
## 梯度下降
### 1.批量梯度下降法BGD（Batch Gradient Descent）
  在更新参数时使用所有的样本来进行更新。假定我们有n个样本，求梯度的时候就采用所有n个样本的梯度数据。
### 2.随机梯度下降法SGD（Stochastic Gradient Descent）
  在更新参数的时候，选取一个样本j来求梯。
  批量梯度下降和随机梯度下降是两个极端，前者采用所有样本数据来进行梯度更新，后者仅采用一个样本来进行梯度更新。
  
  * 训练速度：随机梯度下降训练速度很快，批量梯度下降训练速度很慢； 
  * 准确度：随机梯度下降法仅仅用一个样本决定梯度方向，导致解很有可能不是最优。 
  * 收敛速度：随机梯度下降法每次迭代梯度方向变化很大，不能很快的收敛到局部最优解。
### 3.小批量梯度下降法MBGD（Mini-batch Gradient Descent）
  从所有的n个样本中，选取m个样本进行梯度更新，1 < m < n, m个样本就组成一个Min-batch。小批量梯度下降法兼顾BGD和SGD的优点，在训练速度和收敛速度取得很好的折衷。
## 牛顿法
  用于求解非线性方程组的解以及无约束优化问题。
### 1. 方程组的解
  一阶泰勒展开
### 2. 无约束优化优化问题
  二阶泰勒展开
## 拟牛顿法
## 极大似然估计
## 拉格朗日乘子（KKT条件）
# 非凸优化（重点）
## 缩放函数
* 如何设计缩放函数，将非凸优化转换为凸优化？
