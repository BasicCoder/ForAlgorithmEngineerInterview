# RNN
* LSTM
* GRU
# CNN
* depth-wise 
# 图神经网络
# 优化器
* 作用
* 分类
  - SGDM
  - Adam
# BN(Batch Normal)
# Dropout
  可以有效防止过拟合的发生，在一定程度上达到正则化的效果。
  假定模型在训练时，p概率的激活值被保留，即，1-p概率的激活值被置为0。为了保证输入输出分布的一致性，那么在训练时，需要降Dropout 的输出进行缩放，保证期望E(y)的一致性。在Dropout 之后，E(y) = p * x + (1-p) * 0 = p * x; 那么对输出进行缩放 1 / p 倍。
  
  或者同样的，我们可以在测试的时候对输出进行缩放p 倍。
# 激活函数
# Attention
* 常用的 attention 机制
## 1.Self-Attention
   Attention(Q, K, V) = softmax(Q^T * K / sqrt(d)) * V
# GAN
# Softmax
